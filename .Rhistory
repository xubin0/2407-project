lg_step <- step(lg_full, direction = "backward")
# Extract and sort absolute values of coefficients
coef_abs <- abs(coef(lg_step))
sorted_vars <- sort(coef_abs, decreasing = TRUE)
top_vars <- names(sorted_vars)
# Print top 20 features (excluding intercept)
top_vars[top_vars != "(Intercept)"][1:20]
selected_variables <- c("JobRole", "OverTime", "BusinessTravel", "EducationField", "MaritalStatus",
"JobInvolvement", "EnvironmentSatisfaction", "JobSatisfaction", "Gender",
"WorkLifeBalance", "StockOptionLevel", "RelationshipSatisfaction", "TrainingTimesLastYear",
"NumCompaniesWorked")
data_selected <- data[, c("Attrition", selected_variables), with = FALSE]
data_selected[, Attrition := ifelse(Attrition == "Yes", 1, 0)]
str(data_selected)
# ======================= Rose Sampling ==========================================================
set.seed(2025)
#Train Test Set
split <- sample.split(data_selected$Attrition, SplitRatio = 0.7)
train <- data_selected[split == TRUE]
test <- data_selected[split == FALSE]
# Create tables
before <- table(train$Attrition)
after<- table(train_rose$Attrition)
# Print with aligned formatting
cat("Before ROSE:\n")
cat(sprintf("No  : %d\n", before[["0"]]))
cat(sprintf("Yes : %d\n\n", before[["1"]]))
cat("After ROSE:\n")
cat(sprintf("No  : %d\n", after_tbl[["0"]]))
cat(sprintf("No  : %d\n", after[["0"]]))
cat(sprintf("Yes : %d\n", after[["1"]]))
after<- table(train_rose$Attrition)
# Print with aligned formatting
cat("Before ROSE:\n")
cat(sprintf("No  : %d\n", before[["0"]]))
cat(sprintf("Yes : %d\n\n", before[["1"]]))
cat("After ROSE:\n")
cat(sprintf("No  : %d\n", after[["0"]]))
after<- table(train_rose$Attrition)
train <- data_selected[split == TRUE]
test <- data_selected[split == FALSE]
train_rose <- ROSE(Attrition ~ ., data = train, seed = 2025)$data
# Create tables
before <- table(train$Attrition)
after<- table(train_rose$Attrition)
# Print with aligned formatting
cat("Before ROSE:\n")
cat(sprintf("No  : %d\n", before[["0"]]))
cat(sprintf("Yes : %d\n\n", before[["1"]]))
cat("After ROSE:\n")
cat(sprintf("No  : %d\n", after[["0"]]))
cat(sprintf("Yes : %d\n", after[["1"]]))
cat(sprintf("No  : %d\n", before[["0"]]))
cat(sprintf("Yes : %d\n\n", before[["1"]]))
cat("After ROSE:\n")
cat(sprintf("No  : %d\n", after[["0"]]))
cat(sprintf("Yes : %d\n", after[["1"]]))
#================================================== RandomForest==========================================
set.seed(2025)
p <- ncol(train_rose) - 1
p
View(train_rose)
#================================================== RandomForest==========================================
set.seed(2025)
p <- ncol(train_rose) - 1
p
RF1<- randomForest(Attrition ~ ., data = train_rose, importance=T)
print(RF1)
best_mse <- Inf          # Initialize best MSE
best_rf <- NULL          # To store the best model
best_params <- c()       # Optional: store best mtry and ntree
for (i in 1:p) {
for (j in seq(50, 1000, 50)) {
RF2 <- randomForest(Attrition ~ ., data = train_rose,
importance = TRUE, ntree = j, mtry = i)
print(RF2)
current_mse <- RF2$mse[RF2$ntree]
if (current_mse < best_mse) {
best_mse <- current_mse
best_rf <- RF2
best_params <- c(mtry = i, ntree = j)
}
}
}
RF1<- randomForest(Attrition ~ ., data = train_rose, importance=T)
print(RF1)
best_mse <- Inf          # Initialize best MSE
best_rf <- NULL          # To store the best model
best_params <- c()       # Optional: store best mtry and ntree
#================================================== RandomForest==========================================
set.seed(2025)
p <- ncol(train_rose) - 1
p
RF1<- randomForest(Attrition ~ ., data = train_rose, importance=T)
print(RF1)
best_mse <- Inf          # Initialize best MSE
best_rf <- NULL          # To store the best model
best_params <- c()       # Optional: store best mtry and ntree
for (i in 1:p) {
for (j in seq(50, 800, 50)) {
RF2 <- randomForest(Attrition ~ ., data = train_rose,
importance = TRUE, ntree = j, mtry = i)
print(RF2)
current_mse <- RF2$mse[RF2$ntree]
if (current_mse < best_mse) {
best_mse <- current_mse
best_rf <- RF2
best_params <- c(mtry = i, ntree = j)
}
}
}
warnings()
summary(train_rose)
str(data_selected)
# 1. Convert target variable 'Attrition' to factor (for classification)
data_selected[, Attrition := factor(Attrition, levels = c(0, 1), labels = c("No", "Yes"))]
# 2. Ensure all categorical variables are factors
cat_vars <- c("JobRole", "OverTime", "BusinessTravel", "EducationField", "MaritalStatus", "Gender")
data_selected[, (cat_vars) := lapply(.SD, as.factor), .SDcols = cat_vars]
str(data_selected)
# 1. Convert target variable 'Attrition' to factor (for classification)
data_selected[, Attrition := factor(Attrition, levels = c(0, 1), labels = c("No", "Yes"))]
# 2. Ensure all categorical variables are factors
cat_vars <- c("JobRole", "OverTime", "BusinessTravel", "EducationField", "MaritalStatus", "Gender")
data_selected[, (cat_vars) := lapply(.SD, as.factor), .SDcols = cat_vars]
str(data_selected)  # Check structure again to confirm
View(best_rf)
View(data_selected)
library(data.table)
library(car)
library(ggplot2)
library(caTools)
library(rpart)
library(rpart.plot)
library(randomForest)
library(earth)
library(caret)
library(mltools)
library(dplyr)
library(knitr)
library(ROSE)
setwd("/Users/xb/Desktop/Uni Notes/Y2S2/BC2407/project")
set.seed(2025)
data<-fread("WA_Fn-UseC_-HR-Employee-Attrition.csv", stringsAsFactors = TRUE)
dim(data) #1470 rows, 35 columns
data_encoded<-fread("WA_Fn-UseC_-HR-Employee-Attrition.csv", stringsAsFactors = TRUE)
#identified columns stored as char, will converted them to factor
summary(data)
# Checking for missing values
na_count <- colSums(is.na(data))
print(na_count) # No missing values found
##reason to remove columns:
#reasons to remove column:
sum(data[,Over18!="Y"]) #this column only have 1 value since it is 0, proved it only has Y
length(unique(data[,data$EmployeeCount])) #only 1 unique value
length(unique(data[,data$StandardHours])) #only 1 unique value
# Removing unnecessary columns
cols_to_remove <- c("Over18", "EmployeeCount", "StandardHours")
data <- data[, !cols_to_remove, with=FALSE]
# Convert character columns to factor
char_cols <- names(data)[sapply(data, is.character)]
data[, (char_cols) := lapply(.SD, as.factor), .SDcols = char_cols]
summary(data)
cols_to_remove <- c("Over18", "EmployeeCount", "StandardHours")
data_encoded <- data_encoded[, !cols_to_remove, with = FALSE]
# Binary encoding for Yes/No or Male/Female
data_encoded[, Attrition := ifelse(Attrition == "Yes", 1, 0)]
data_encoded[, Gender := ifelse(Gender == "Male", 1, 0)]
data_encoded[, OverTime := ifelse(OverTime == "Yes", 1, 0)]
# One-hot encode selected categorical columns
data_encoded <- one_hot(as.data.table(data_encoded),
cols = c("BusinessTravel", "Department", "EducationField",
"JobRole", "MaritalStatus"),
sparsifyNAs = FALSE)
drop_dummies <- c(
"Department_Research & Development",         # base for Department
"EducationField_Life Sciences",              # base for EducationField
"JobRole_Sales Executive",                   # base for JobRole
"MaritalStatus_Married",                     # base for Marital Status
"BusinessTravel_Travel_Rarely"               # base for BusinessTravel
)
data_encoded <- data_encoded[, !drop_dummies, with = FALSE]
# Convert any logical columns to numeric
logical_cols <- names(data_encoded)[sapply(data_encoded, is.logical)]
data_encoded[, (logical_cols) := lapply(.SD, as.integer), .SDcols = logical_cols]
#identify all categorical and numeric data
categorical_vars <- names(data)[sapply(data, function(col) is.character(col) || is.factor(col))]
continuous_vars <- names(data)[sapply(data, is.numeric)]
# Split the data into categorical and continuous
cat_data <- data[, ..categorical_vars]
num_data <- data[, ..continuous_vars]
# Print the names of the variables
cat("Categorical Variables:\n", categorical_vars, "\n\n")
cat("Continuous Variables:\n", continuous_vars, "\n\n")
dim(cat_data) #8 columns of categorical variable
dim(num_data) #24 columns of numerical variable
head(cat_data)
head(num_data)
#get dimensions to do plotting
cat.dim<- c(ceiling(sqrt(ncol(cat_data))), ceiling(ncol(cat_data)/ceiling(sqrt(ncol(cat_data)))))
#get dimensions to do plotting
num.dim<- c(ceiling(sqrt(ncol(num_data))),ceiling(ncol(num_data)/ceiling(sqrt(ncol(num_data)))))
#plot all categorical variables
par(mfrow=cat.dim)
for (i in 1:ncol(cat_data)) {
barplot(table(cat_data[[i]]), main = colnames(cat_data)[i], xlab = colnames(cat_data)[i], ylab = "Frequency")
}
#plot all numerical variables
par(mfrow=c(3,3)) #used 3x3 such that graphs can be bigger
for(i in 1:ncol(num_data)){
boxplot(num_data[[i]], main= colnames(num_data)[i])
}
unique(data[,data$PerformanceRating])
#plotting response against numerical var
for (i in 1:ncol(num_data)) {
p <- ggplot(data, aes(x = Attrition, y = num_data[[i]])) +
geom_violin(fill = "skyblue", trim = FALSE) +
geom_boxplot(width = 0.1, fill = "white") +
ggtitle(colnames(num_data)[i]) +
ylab(colnames(num_data)[i]) +
xlab("Attrition")
print(p)
}
#plotting response against categorical var
for (i in 1:ncol(cat_data)) {
var_name <- colnames(cat_data)[i]
if (var_name != "Attrition") {  # Skip response itself
p <- ggplot(data, aes_string(x = var_name, fill = "Attrition")) +
geom_bar(position = "fill") +
ylab("Proportion") +
ggtitle(paste(var_name, "vs Attrition")) +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
print(p)
}
}
# Logistic Regression
lg_full <- glm(Attrition ~ ., data = data_encoded, family = "binomial")
# Check the summary of the model
summary(lg_full)
# Check for multicollinearity > 10
vif(lg_full)
# Perform backward stepwise selection based on AIC
lg_step <- step(lg_full, direction = "backward")
# Extract and sort absolute values of coefficients
coef_abs <- abs(coef(lg_step))
sorted_vars <- sort(coef_abs, decreasing = TRUE)
top_vars <- names(sorted_vars)
# Print top 20 features (excluding intercept)
top_vars[top_vars != "(Intercept)"][1:20]
selected_variables <- c("JobRole", "OverTime", "BusinessTravel", "EducationField", "MaritalStatus",
"JobInvolvement", "EnvironmentSatisfaction", "JobSatisfaction", "Gender",
"WorkLifeBalance", "StockOptionLevel", "RelationshipSatisfaction", "TrainingTimesLastYear",
"NumCompaniesWorked")
data_selected <- data[, c("Attrition", selected_variables), with = FALSE]
data_selected[, Attrition := ifelse(Attrition == "Yes", 1, 0)]
str(data_selected)
library(data.table)
library(car)
library(ggplot2)
library(caTools)
library(rpart)
library(rpart.plot)
library(randomForest)
library(earth)
library(caret)
library(mltools)
library(dplyr)
library(knitr)
library(ROSE)
setwd("/Users/xb/Desktop/Uni Notes/Y2S2/BC2407/project")
set.seed(2025)
data<-fread("WA_Fn-UseC_-HR-Employee-Attrition.csv", stringsAsFactors = TRUE)
dim(data) #1470 rows, 35 columns
data_encoded<-fread("WA_Fn-UseC_-HR-Employee-Attrition.csv", stringsAsFactors = TRUE)
#identified columns stored as char, will converted them to factor
summary(data)
# Checking for missing values
na_count <- colSums(is.na(data))
print(na_count) # No missing values found
##reason to remove columns:
#reasons to remove column:
sum(data[,Over18!="Y"]) #this column only have 1 value since it is 0, proved it only has Y
length(unique(data[,data$EmployeeCount])) #only 1 unique value
length(unique(data[,data$StandardHours])) #only 1 unique value
# Removing unnecessary columns
cols_to_remove <- c("Over18", "EmployeeCount", "StandardHours")
data <- data[, !cols_to_remove, with=FALSE]
# Convert character columns to factor
char_cols <- names(data)[sapply(data, is.character)]
data[, (char_cols) := lapply(.SD, as.factor), .SDcols = char_cols]
summary(data)
cols_to_remove <- c("Over18", "EmployeeCount", "StandardHours")
data_encoded <- data_encoded[, !cols_to_remove, with = FALSE]
# Binary encoding for Yes/No or Male/Female
data_encoded[, Attrition := ifelse(Attrition == "Yes", 1, 0)]
data_encoded[, Gender := ifelse(Gender == "Male", 1, 0)]
data_encoded[, OverTime := ifelse(OverTime == "Yes", 1, 0)]
# One-hot encode selected categorical columns
data_encoded <- one_hot(as.data.table(data_encoded),
cols = c("BusinessTravel", "Department", "EducationField",
"JobRole", "MaritalStatus"),
sparsifyNAs = FALSE)
drop_dummies <- c(
"Department_Research & Development",         # base for Department
"EducationField_Life Sciences",              # base for EducationField
"JobRole_Sales Executive",                   # base for JobRole
"MaritalStatus_Married",                     # base for Marital Status
"BusinessTravel_Travel_Rarely"               # base for BusinessTravel
)
data_encoded <- data_encoded[, !drop_dummies, with = FALSE]
# Convert any logical columns to numeric
logical_cols <- names(data_encoded)[sapply(data_encoded, is.logical)]
data_encoded[, (logical_cols) := lapply(.SD, as.integer), .SDcols = logical_cols]
#identify all categorical and numeric data
categorical_vars <- names(data)[sapply(data, function(col) is.character(col) || is.factor(col))]
continuous_vars <- names(data)[sapply(data, is.numeric)]
# Split the data into categorical and continuous
cat_data <- data[, ..categorical_vars]
num_data <- data[, ..continuous_vars]
# Print the names of the variables
cat("Categorical Variables:\n", categorical_vars, "\n\n")
cat("Continuous Variables:\n", continuous_vars, "\n\n")
dim(cat_data) #8 columns of categorical variable
dim(num_data) #24 columns of numerical variable
head(cat_data)
head(num_data)
#get dimensions to do plotting
cat.dim<- c(ceiling(sqrt(ncol(cat_data))), ceiling(ncol(cat_data)/ceiling(sqrt(ncol(cat_data)))))
#get dimensions to do plotting
num.dim<- c(ceiling(sqrt(ncol(num_data))),ceiling(ncol(num_data)/ceiling(sqrt(ncol(num_data)))))
#plot all categorical variables
par(mfrow=cat.dim)
for (i in 1:ncol(cat_data)) {
barplot(table(cat_data[[i]]), main = colnames(cat_data)[i], xlab = colnames(cat_data)[i], ylab = "Frequency")
}
#plot all numerical variables
par(mfrow=c(3,3)) #used 3x3 such that graphs can be bigger
for(i in 1:ncol(num_data)){
boxplot(num_data[[i]], main= colnames(num_data)[i])
}
unique(data[,data$PerformanceRating])
#plotting response against numerical var
for (i in 1:ncol(num_data)) {
p <- ggplot(data, aes(x = Attrition, y = num_data[[i]])) +
geom_violin(fill = "skyblue", trim = FALSE) +
geom_boxplot(width = 0.1, fill = "white") +
ggtitle(colnames(num_data)[i]) +
ylab(colnames(num_data)[i]) +
xlab("Attrition")
print(p)
}
#plotting response against categorical var
for (i in 1:ncol(cat_data)) {
var_name <- colnames(cat_data)[i]
if (var_name != "Attrition") {  # Skip response itself
p <- ggplot(data, aes_string(x = var_name, fill = "Attrition")) +
geom_bar(position = "fill") +
ylab("Proportion") +
ggtitle(paste(var_name, "vs Attrition")) +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
print(p)
}
}
# Logistic Regression
lg_full <- glm(Attrition ~ ., data = data_encoded, family = "binomial")
# Check the summary of the model
summary(lg_full)
# Check for multicollinearity > 10
vif(lg_full)
# Perform backward stepwise selection based on AIC
lg_step <- step(lg_full, direction = "backward")
# Extract and sort absolute values of coefficients
coef_abs <- abs(coef(lg_step))
sorted_vars <- sort(coef_abs, decreasing = TRUE)
top_vars <- names(sorted_vars)
# Print top 20 features (excluding intercept)
top_vars[top_vars != "(Intercept)"][1:20]
selected_variables <- c("JobRole", "OverTime", "BusinessTravel", "EducationField", "MaritalStatus",
"JobInvolvement", "EnvironmentSatisfaction", "JobSatisfaction", "Gender",
"WorkLifeBalance", "StockOptionLevel", "RelationshipSatisfaction", "TrainingTimesLastYear",
"NumCompaniesWorked")
data_selected <- data[, c("Attrition", selected_variables), with = FALSE]
data_selected[, Attrition := ifelse(Attrition == "Yes", 1, 0)]
str(data_selected)
selected_variables <- c("JobRole", "OverTime", "BusinessTravel", "EducationField", "MaritalStatus",
"JobInvolvement", "EnvironmentSatisfaction", "JobSatisfaction", "Gender",
"WorkLifeBalance", "StockOptionLevel", "RelationshipSatisfaction", "TrainingTimesLastYear",
"NumCompaniesWorked")
data_selected <- data[, c("Attrition", selected_variables), with = FALSE]
str(data_selected)
data_selected[, Attrition := ifelse(Attrition == "Yes", 1, 0)]
str(data_selected)
selected_variables <- c("JobRole", "OverTime", "BusinessTravel", "EducationField", "MaritalStatus",
"JobInvolvement", "EnvironmentSatisfaction", "JobSatisfaction", "Gender",
"WorkLifeBalance", "StockOptionLevel", "RelationshipSatisfaction", "TrainingTimesLastYear",
"NumCompaniesWorked")
data_selected <- data[, c("Attrition", selected_variables), with = FALSE]
str(data_selected)
View(data)
View(data_selected)
# ======================= Rose Sampling ==========================================================
set.seed(2025)
#Train Test Set
split <- sample.split(data_selected$Attrition, SplitRatio = 0.7)
train <- data_selected[split == TRUE]
test <- data_selected[split == FALSE]
train_rose <- ROSE(Attrition ~ ., data = train, seed = 2025)$data
# Create tables
before <- table(train$Attrition)
after<- table(train_rose$Attrition)
# Print with aligned formatting
cat("Before ROSE:\n")
cat(sprintf("No  : %d\n", before[["0"]]))
#Train Test Set
split <- sample.split(data_selected$Attrition, SplitRatio = 0.7)
train <- data_selected[split == TRUE]
test <- data_selected[split == FALSE]
train_rose <- ROSE(Attrition ~ ., data = train, seed = 2025)$data
# Create tables
before <- table(train$Attrition)
after<- table(train_rose$Attrition)
# Print with aligned formatting
cat("Before ROSE:\n")
cat(sprintf("No  : %d\n", before[["0"]]))
View(train_rose)
cat(sprintf("No  : %d\n", before[["No"]]))
cat(sprintf("Yes : %d\n\n", before[["Yes"]]))
cat("After ROSE:\n")
cat(sprintf("No  : %d\n", after[["No"]]))
cat(sprintf("Yes : %d\n", after[["Yes"]]))
summary(train_rose)
#================================================== RandomForest==========================================
set.seed(2025)
p <- ncol(train_rose) - 1
p
RF1<- randomForest(Attrition ~ ., data = train_rose, importance=T)
print(RF1)
best_mse <- Inf          # Initialize best MSE
#================================================== RandomForest==========================================
set.seed(2025)
p <- ncol(train_rose) - 1  # number of predictors (excluding target)
# To store results
results <- data.table(mtry = integer(), ntree = integer(), oob_error = numeric())
# To store best model
best_error <- Inf
best_rf <- NULL
best_params <- c()
# Grid search over mtry and ntree
for (i in 1:p) {
for (j in seq(50, 800, 50)) {
RF2 <- randomForest(Attrition ~ ., data = train_rose,
importance = TRUE, ntree = j, mtry = i)
current_error <- RF2$err.rate[RF2$ntree, "OOB"]
# Store current result
results <- rbind(results, data.table(mtry = i, ntree = j, oob_error = current_error))
# Update best model if current is better
if (current_error < best_error) {
best_error <- current_error
best_rf <- RF2
best_params <- c(mtry = i, ntree = j)
}
}
}
#================================================== RandomForest==========================================
set.seed(2025)
p <- ncol(train_rose) - 1  # number of predictors (excluding target)
# To store results
results <- data.table(mtry = integer(), ntree = integer(), oob_error = numeric())
# To store best model
best_error <- Inf
best_rf <- NULL
best_params <- c()
# Grid search over mtry and ntree
for (i in 1:p) {
for (j in seq(50, 800, 50)) {
start_time <- Sys.time()
RF2 <- randomForest(Attrition ~ ., data = train_rose,
importance = TRUE, ntree = j, mtry = i)
current_error <- RF2$err.rate[RF2$ntree, "OOB"]
# Store current result
results <- rbind(results, data.table(mtry = i, ntree = j, oob_error = current_error))
# Update best model if current is better
if (current_error < best_error) {
best_error <- current_error
best_rf <- RF2
best_params <- c(mtry = i, ntree = j)
}
end_time <- Sys.time()
print(paste0("Tested mtry = ", i, ", ntree = ", j,
" | OOB Error = ", round(current_error, 4),
" | Time: ", round(difftime(end_time, start_time, units = "secs"), 2), "s"))
}
}
# Final output
print(results)
print(paste("Best params - mtry:", best_params["mtry"], "ntree:", best_params["ntree"]))
fwrite(results, "rf_grid_search_results.csv")
RF_best <- RF2 <- randomForest(Attrition ~ ., data = train_rose,
importance = TRUE, ntree = 550, mtry = 3)
print(RF_best)
predicted <- predict(RF_best, newdata = test)
confusionMatrix(predicted, test$Attrition)
library(pROC)
# Get probabilities of "Yes" class
probs <- predict(RF_best, newdata = test, type = "prob")
# Compute ROC and AUC
roc_obj <- roc(response = test$Attrition, predictor = probs[, "Yes"])
auc(roc_obj)  # Print AUC
plot(roc_obj, col = "blue", main = "ROC Curve")
plot(roc_obj, col = "blue", main = "ROC Curve")
auc(roc_obj)  # Print AUC
plot(roc_obj, col = "blue", main = "ROC Curve")
